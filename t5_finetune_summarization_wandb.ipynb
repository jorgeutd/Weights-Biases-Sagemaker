{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune a T5 transformer for text summarization\n",
    "# Register project and result in Weight & Bias WandB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging into W&B\n",
    "Goto your profile page and copy an api key. Add a new cell below and run the following:\n",
    "\n",
    "!wandb login PASTE_API_KEY_HERE\n",
    "\n",
    "You only need to run this once as it writes your credentials in your home directory.\n",
    "\n",
    "W&B looks for a file named `secrets.env` relative to the training script and loads them into the environment when wandb.init() is called. You can generate a `secrets.env` file by calling wandb.sagemaker_auth(path=\"source_dir\") in the script you use to launch your experiments. Be sure to add this file to your .gitignore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (0.10.10)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (3.1.11)\n",
      "Requirement already satisfied: Click>=7.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (7.0)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (0.19.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (2.22.0)\n",
      "Requirement already satisfied: watchdog>=0.8.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (0.10.2)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (3.14.0)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (5.0.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (5.6.7)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: urllib3>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sentry-sdk>=0.4.0->wandb) (1.25.10)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sentry-sdk>=0.4.0->wandb) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: pathtools>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "#!wandb login <API_KEY>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "#!wandb login --relogin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries and initialize SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::223817798831:role/service-role/AmazonSageMaker-ExecutionRole-20200708T194212\n",
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "# Create a SageMaker session to work with\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# Get the role of our user and the region\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "print(role)\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    }
   ],
   "source": [
    "wandb.sagemaker_auth(path=\"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload the data to S3\n",
    "\n",
    "Now, we will need to upload the training dataset to S3 in order for our training code to access it. we upload the training data to the SageMaker S3 bucket so that we can provide access to it while training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the variables for data locations\n",
    "data_folder_name='data'\n",
    "train_filename = 'news_summary.csv'\n",
    "# Set the absolute path of the train data \n",
    "train_file = os.path.abspath(os.path.join(data_folder_name, train_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your bucket name\n",
    "bucket_name = 'edumunozsala-ml-sagemaker'\n",
    "# Set the training data folder in S3\n",
    "training_folder = r't5-summarization/train'\n",
    "# Set the output folder in S3\n",
    "output_folder = r't5-summarization'\n",
    "# Set the checkpoint in S3 folder for our model \n",
    "#ckpt_folder = r't5-summarization/ckpt'\n",
    "\n",
    "training_data_uri = r's3://' + bucket_name + r'/' + training_folder\n",
    "output_data_uri = r's3://' + bucket_name + r'/' + output_folder\n",
    "#ckpt_data_uri = r's3://' + bucket_name + r'/' + ckpt_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3://edumunozsala-ml-sagemaker/t5-summarization/train',\n",
       " 's3://edumunozsala-ml-sagemaker/t5-summarization')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_uri,output_data_uri #,ckpt_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(train_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The cell above uploads the entire contents of our data directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build and Train the PyTorch Model\n",
    "\n",
    "A model in the SageMaker framework, in particular, comprises three objects:\n",
    "\n",
    " - Model Artifacts,\n",
    " - Training Code, and\n",
    " - Inference Code,\n",
    " \n",
    "each of which interact with one another.\n",
    "\n",
    "We will start by implementing our own neural network in PyTorch along with a training script. For the purposes of this project we need to provide the model object implementation in the `model.py` file, inside of the `train` folder. You can see the provided implementation by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show code of trayn.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to construct a PyTorch model using SageMaker we must provide SageMaker with a training script. We may optionally include a directory which will be copied to the container and from which our training code will be run. When the training container is executed it will check the uploaded directory (if there is one) for a `requirements.txt` file and install any required Python libraries, after which the training script will be run.\n",
    "\n",
    "In this example, we require the packages: numpy, pandas, transformers, wandb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model in a SageMaker training job\n",
    "\n",
    "When a PyTorch model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained. Inside of the `train` directory is a file called `train.py` which contains most of the necessary code to train our model. \n",
    "\n",
    "The way that SageMaker passes hyperparameters to the training script is by way of arguments. These arguments can then be parsed and used in the training script. To see how this is done take a look at the provided `train/train.py` file.\n",
    "\n",
    "First, we need to set which type of instance will run our training:\n",
    "- Local: We do not launch a real compute instance, just a container where our scripts will run. This scenario is very useful to test that the train script is working fine because it is faster to run a container than an compute instance. But finally, when we confirm that everything is working we must change the instance type for a \"real\" training instance.\n",
    "- ml.m4.4xlarge: It is a CPU instance\n",
    "- ml.p2.xlarge: A GPU instance to use when managing a big volume of data to train on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the type of instance to use for training\n",
    "#instance_type='ml.m4.4xlarge' # CPU instance\n",
    "instance_type='ml.p2.xlarge' # GPU instance\n",
    "#instance_type='local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='source',\n",
    "                    role=role,\n",
    "                    framework_version='1.4.0',\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type=instance_type,\n",
    "                    output_path=output_data_uri,\n",
    "                    code_location=output_data_uri,\n",
    "                    base_job_name='t5-summarization',\n",
    "                    hyperparameters={\n",
    "                        'train_epochs': 1,\n",
    "                        'datafile': 'news_summary.csv'\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5-summarization-2020-11-14-19-24-45\n"
     ]
    }
   ],
   "source": [
    "# Set the job name and show it\n",
    "base_job_name='t5-summarization'\n",
    "job_name = '{}-{}'.format(base_job_name,time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()))\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-14 19:24:47 Starting - Starting the training job...\n",
      "2020-11-14 19:24:50 Starting - Launching requested ML instances......\n",
      "2020-11-14 19:26:04 Starting - Preparing the instances for training......\n",
      "2020-11-14 19:27:18 Downloading - Downloading input data......\n",
      "2020-11-14 19:28:15 Training - Downloading the training image............\n",
      "2020-11-14 19:30:05 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-11-14 19:30:07,254 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-11-14 19:30:07,278 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-11-14 19:30:07,283 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-11-14 19:30:07,676 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-11-14 19:30:07,676 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-11-14 19:30:07,676 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-11-14 19:30:07,676 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpn1e_ab2a/module_dir\u001b[0m\n",
      "\u001b[34mCollecting wandb\u001b[0m\n",
      "\u001b[34m  Downloading wandb-0.10.10-py2.py3-none-any.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting transformers\n",
      "  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.25.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\u001b[0m\n",
      "\u001b[34mCollecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.11-py3-none-any.whl (159 kB)\u001b[0m\n",
      "\u001b[34mCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting configparser>=3.8.1\n",
      "  Downloading configparser-5.0.1-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (5.7.0)\u001b[0m\n",
      "\u001b[34mCollecting watchdog>=0.8.3\n",
      "  Downloading watchdog-0.10.3.tar.gz (94 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (5.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=0.4.0\n",
      "  Downloading sentry_sdk-0.19.3-py2.py3-none-any.whl (128 kB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (4.42.1)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece==0.1.91\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (20.4)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.9.3\n",
      "  Downloading tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 2)) (0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 4)) (2020.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.12.0->wandb->-r requirements.txt (line 1)) (46.4.0.post20200518)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.5-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mCollecting pathtools>=0.1.1\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (1.25.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 2)) (0.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mCollecting smmap<4,>=3.0.1\n",
      "  Downloading smmap-3.0.4-py2.py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name, subprocess32, promise, watchdog, sacremoses, pathtools\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=11880 sha256=48ba270521260064bf2427ec62e6906e3262f2fa8fa9f8def4bcb8cdbae215a9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3345y3dl/wheels/e4/f5/b5/ce0d4a7999f4f14c1517144c576e154a9f79086b3c51214e89\n",
      "  Building wheel for subprocess32 (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for subprocess32 (setup.py): finished with status 'done'\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6489 sha256=3ed3b298209c19cb9ee6a2dd1913293344f506c4e4a7f3095ca857bb22098059\n",
      "  Stored in directory: /root/.cache/pip/wheels/44/3a/ab/102386d84fe551b6cedb628ed1e74c5f5be76af8b909aeda09\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=955ba6e59356bf7b4b555d36ebe7589386de5a2f6373441ca63bb683d642fdc5\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "  Building wheel for watchdog (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for watchdog (setup.py): finished with status 'done'\n",
      "  Created wheel for watchdog: filename=watchdog-0.10.3-py3-none-any.whl size=73871 sha256=ccd65a338fbcef6bb308ade5f0803b6faaccfefe3cda40ea2e856d98128bd6dc\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/d5/fc/fddf5870c0fb0b755abdf41f4889ef54742ee21fff693aae22\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=22f0a11fe5fc322fb3aed4238f10807ac8c437cdc2ef60ab8c3f3568251b5045\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "  Building wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8784 sha256=dd75f4796f06d278f571d6055bc2374cf90943c23e4ddec29e8ac41b3acdccdc\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/ea/90/e37d463fb3b03848bf715080595de62545266f53dd546b2497\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name subprocess32 promise watchdog sacremoses pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: shortuuid, docker-pycreds, subprocess32, smmap, gitdb, GitPython, promise, configparser, pathtools, watchdog, sentry-sdk, wandb, regex, sacremoses, sentencepiece, filelock, tokenizers, transformers, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.11 configparser-5.0.1 default-user-module-name-1.0.0 docker-pycreds-0.4.0 filelock-3.0.12 gitdb-4.0.5 pathtools-0.1.2 promise-2.3 regex-2020.11.13 sacremoses-0.0.43 sentencepiece-0.1.91 sentry-sdk-0.19.3 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 tokenizers-0.9.3 transformers-3.5.1 wandb-0.10.10 watchdog-0.10.3\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1.1; however, version 20.2.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-11-14 19:30:21,592 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_epochs\": 1,\n",
      "        \"datafile\": \"news_summary.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"t5-summarization-2020-11-14-19-24-45\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://edumunozsala-ml-sagemaker/t5-summarization/t5-summarization-2020-11-14-19-24-45/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"datafile\":\"news_summary.csv\",\"train_epochs\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://edumunozsala-ml-sagemaker/t5-summarization/t5-summarization-2020-11-14-19-24-45/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"datafile\":\"news_summary.csv\",\"train_epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"t5-summarization-2020-11-14-19-24-45\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://edumunozsala-ml-sagemaker/t5-summarization/t5-summarization-2020-11-14-19-24-45/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--datafile\",\"news_summary.csv\",\"--train_epochs\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_DATAFILE=news_summary.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --datafile news_summary.csv --train_epochs 1\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mGet train and validation dataloaders.\u001b[0m\n",
      "\u001b[34mFULL Dataset: (500, 2)\u001b[0m\n",
      "\u001b[34mTRAIN Dataset: (450, 2)\u001b[0m\n",
      "\u001b[34mTEST Dataset: (50, 2)\u001b[0m\n",
      "\u001b[34mCreating the pretrained model\u001b[0m\n",
      "\u001b[34mActivating WandB tracking\u001b[0m\n",
      "\u001b[34mInitiating Fine-Tuning for the model on our dataset\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:55.596 algo-1:72 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:55.596 algo-1:72 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:55.596 algo-1:72 INFO hook.py:236] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:55.596 algo-1:72 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:55.624 algo-1:72 INFO hook.py:376] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:55.624 algo-1:72 INFO hook.py:437] Hook is writing from the hook with pid: 72\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.475 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.0.layer.0.SelfAttention NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.476 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.0.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.484 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.487 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.1.layer.0.SelfAttention NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.488 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.1.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.490 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.1 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.493 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.2.layer.0.SelfAttention NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.494 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.2.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.497 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.2 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.499 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.3.layer.0.SelfAttention NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.500 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.3.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.502 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.3 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.505 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.4.layer.0.SelfAttention NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.506 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.4.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.508 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.4 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.511 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.5.layer.0.SelfAttention NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.512 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.5.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.515 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.5 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.518 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.6.layer.0.SelfAttention NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.518 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.6.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.521 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.6 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.524 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.7.layer.0.SelfAttention NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.524 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.7.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.527 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.7 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.529 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.8.layer.0.SelfAttention NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.530 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.8.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.533 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.8 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.536 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.9.layer.0.SelfAttention NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.536 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.9.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.539 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.9 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.542 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.10.layer.0.SelfAttention NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.542 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.10.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.545 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.10 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.548 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.11.layer.0.SelfAttention NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.548 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.11.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:30:56.551 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:encoder.block.11 NoneType\u001b[0m\n",
      "\u001b[34mEpoch: 0, Loss:  3.82955265045166\u001b[0m\n",
      "\u001b[34mNow generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\u001b[0m\n",
      "\u001b[34mCompleted 0\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:34:17.189 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:decoder BaseModelOutputWithPastAndCrossAttentions\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:34:17.190 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:T5ForConditionalGeneration Seq2SeqLMOutput\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:34:52.476 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:decoder BaseModelOutputWithPastAndCrossAttentions\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:34:52.476 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:T5ForConditionalGeneration Seq2SeqLMOutput\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:35:22.974 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:decoder BaseModelOutputWithPastAndCrossAttentions\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:35:22.975 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:T5ForConditionalGeneration Seq2SeqLMOutput\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:35:58.954 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:decoder BaseModelOutputWithPastAndCrossAttentions\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:35:58.955 algo-1:72 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:T5ForConditionalGeneration Seq2SeqLMOutput\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-11-14 19:36:21 Uploading - Uploading generated training model\u001b[34mOutput Files generated for review\u001b[0m\n",
      "\u001b[34m[2020-11-14 19:36:16.835 algo-1:72 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-11-14 19:36:20,729 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-11-14 19:39:09 Completed - Training job completed\n",
      "Training seconds: 711\n",
      "Billable seconds: 711\n"
     ]
    }
   ],
   "source": [
    "# Call the fit method to launch the training job\n",
    "estimator.fit({'training':training_data_uri}, job_name = job_name) \n",
    "#              experiment_config = experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}